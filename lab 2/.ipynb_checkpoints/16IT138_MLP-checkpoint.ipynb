{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from random import shuffle\n",
    "from math import exp\n",
    "from random import randrange\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(10)\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    n_x =X.shape[0]\n",
    "    n_h = 5\n",
    "    n_y = Y.shape[0] \n",
    "    return (n_x, n_h, n_y)\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    W1 = np.full((n_h,n_x), 1/(n_x+1))\n",
    "    b1 = np.full((n_h,1), 1/(n_x+1))\n",
    "    W2 = np.full((n_y,n_h), 1/(n_h+1))\n",
    "    b2 = np.full((n_y,1), 1/(n_h+1))\n",
    "    \n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "   \n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    A2 = sigmoid(Z2)\n",
    "#     print(A2.shape)\n",
    "#     assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    network_vals = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, network_vals\n",
    "\n",
    "def backward_propagation(parameters, networks_vals, X, y):\n",
    "    m = X.shape[1]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    A1 = networks_vals[\"A1\"]\n",
    "    A2 = networks_vals[\"A2\"]\n",
    "   \n",
    "    dZ2 = A2 - y\n",
    "    dW2 = np.dot(dZ2,A1.T)/m\n",
    "    db2 = np.sum(dZ2,axis=1,keepdims=True)/m\n",
    "    dZ1 = np.dot(W2.T,dZ2)*(1-np.power(A1,2))\n",
    "    dW1 = np.dot(dZ1,X.T)/m\n",
    "    db1 = np.sum(dZ1,axis=1,keepdims=True)/m\n",
    "    \n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate =0.1):\n",
    "\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "   \n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, n_h, num_iterations = 1000, print_cost=False,lr=0.1):\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    " \n",
    "    parameters = initialize_parameters(n_x,n_h,n_y)\n",
    "  \n",
    "    for _ in range(0, num_iterations):\n",
    "        _, cache = forward_propagation(X, parameters)\n",
    " \n",
    "        grads = backward_propagation(parameters,cache,X,Y)\n",
    " \n",
    "        parameters = update_parameters(parameters,grads,lr)\n",
    "    return parameters\n",
    "\n",
    "def predict(parameters, X): \n",
    "    A2, _ = forward_propagation(X,parameters)\n",
    "    predictions=[1 if A2[0][i] > 0.6 else 0 for i in range(A2.shape[1])] \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "\n",
    "def metrics(y_pred, y_true):\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        pred = pred[0]\n",
    "#         print(true, pred)\n",
    "        if true == 0:\n",
    "            if pred == 0:\n",
    "                tn += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        else:\n",
    "            if pred == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "    precision, recall = None, None\n",
    "    try:\n",
    "        precision=tp/(tp+fp)\n",
    "        recall=tp/(tp+fn)\n",
    "    except:\n",
    "        print(\"Divide by zero\")    \n",
    "    return precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for _ in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_algorithm(dataset, n_folds,lr=0.1):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    f_acc, f_rec, f_pre = 0., 0., 0.\n",
    "    f=1\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        actual=[]\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            actual.append(row[-1])\n",
    "        train=np.array(train_set)\n",
    "        X_train=train[:,:train.shape[1]-1].T\n",
    "        y_train=train[:,-1].reshape(1,train.shape[0])\n",
    "        test=np.array(test_set)\n",
    "        X_test=test[:,:test.shape[1]-1].T\n",
    "        actual=test[:,-1].reshape(1,test.shape[0])\n",
    "        parameters = model(X_train,y_train,5,lr=lr)\n",
    "        predicted = predict(parameters, X_test)\n",
    "        acc = accuracy(actual.reshape(-1,1), predicted)\n",
    "        precision, recall = metrics(actual.reshape(-1,1),predicted)\n",
    "        f_acc += acc\n",
    "        f_rec += recall\n",
    "        f_pre += precision\n",
    "        print(\"accuracy :\"+str(acc), \", precision :\"+str(precision), \", recall :\"+str(recall))\n",
    "        f+=1\n",
    "    print('Final:')\n",
    "    print(\"accuracy :\"+str(f_acc / len(folds)),\n",
    "          \", precision :\"+str(f_pre / len(folds)),\n",
    "          \", recall :\"+str(f_rec / len(folds)))\n",
    "\n",
    "def get_data(file,y_in):\n",
    "    reader = csv.reader(open(\"./\"+file),delimiter=\",\")\n",
    "    data=[]\n",
    "    c=0\n",
    "    for row in reader:\n",
    "        if(c==0):\n",
    "            c+=1\n",
    "            continue\n",
    "        data.append(row)\n",
    "    random.seed(123)\n",
    "    shuffle(data)\n",
    "    for item in data:\n",
    "        item[-1], item[y_in] = item[y_in], item[-1]\n",
    "\n",
    "    X=[]\n",
    "    y=[]\n",
    "    c=0\n",
    "\n",
    "    for row in data:\n",
    "        X.append(row[:-1])\n",
    "        y.append(row[-1])\n",
    "    unique=[]\n",
    "    for i in range(len(y)):\n",
    "        if y[i] not in unique:\n",
    "            unique.append(y[i])\n",
    "    for i in range(len(y)):\n",
    "        y[i]=unique.index(y[i])\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X[0])):\n",
    "            X[i][j]=float(X[i][j])\n",
    "    for i in range(len(X)):\n",
    "        X[i]=[1]+X[i]\n",
    "    data=[]\n",
    "    for i in range(len(X)):\n",
    "        data.append(X[i]+[y[i]])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For IRIS dataset\n",
      "accuracy :100.0 , precision :1.0 , recall :1.0\n",
      "accuracy :100.0 , precision :1.0 , recall :1.0\n",
      "accuracy :100.0 , precision :1.0 , recall :1.0\n",
      "accuracy :100.0 , precision :1.0 , recall :1.0\n",
      "accuracy :100.0 , precision :1.0 , recall :1.0\n",
      "accuracy :100.0 , precision :1.0 , recall :1.0\n",
      "accuracy :100.0 , precision :1.0 , recall :1.0\n",
      "accuracy :100.0 , precision :1.0 , recall :1.0\n",
      "accuracy :100.0 , precision :1.0 , recall :1.0\n",
      "accuracy :100.0 , precision :1.0 , recall :1.0\n",
      "Final:\n",
      "accuracy :100.0 , precision :1.0 , recall :1.0\n"
     ]
    }
   ],
   "source": [
    "iris=get_data(\"IRIS.csv\",-1)\n",
    "print(\"For IRIS dataset\")\n",
    "evaluate_algorithm(iris,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For SPECT dataset\n",
      "accuracy :73.07692307692307 , precision :0.25 , recall :0.6666666666666666\n",
      "accuracy :88.46153846153845 , precision :0.75 , recall :0.6\n",
      "accuracy :92.3076923076923 , precision :0.75 , recall :0.75\n",
      "accuracy :76.92307692307693 , precision :0.16666666666666666 , recall :0.5\n",
      "accuracy :73.07692307692307 , precision :0.0 , recall :0.0\n",
      "accuracy :88.46153846153845 , precision :0.5 , recall :0.6666666666666666\n",
      "accuracy :80.76923076923077 , precision :0.5 , recall :1.0\n",
      "accuracy :88.46153846153845 , precision :0.5 , recall :0.6666666666666666\n",
      "accuracy :84.61538461538461 , precision :0.4 , recall :0.6666666666666666\n",
      "accuracy :88.46153846153845 , precision :0.6 , recall :0.75\n",
      "Final:\n",
      "accuracy :83.46153846153847 , precision :0.4416666666666667 , recall :0.6266666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"For SPECT dataset\")\n",
    "scept = get_data(\"SPECT.csv\",0)\n",
    "evaluate_algorithm(scept, 10, 0.11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
